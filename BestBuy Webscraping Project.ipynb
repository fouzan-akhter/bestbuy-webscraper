{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b19199",
   "metadata": {},
   "source": [
    "# BestBuy Webscraping Project\n",
    "\n",
    "Author: Muhammad Fouzan Akhter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4b514b",
   "metadata": {},
   "source": [
    "The code for a web scraping project that targets BestBuy is shown below. Underscoring the importance of following website privacy policies is crucial for any online scraping project. It is imperative to highlight that this project is scraping entirely publicly accessible data from Yahoo Finance while adhering to the platform's privacy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd11ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing required packages:\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install pandas\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7ad23d",
   "metadata": {},
   "source": [
    "**This Project is coded in the Jupyter Notebook Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69522402",
   "metadata": {},
   "source": [
    "\n",
    "The webscraper for BestBuy is divided into two parts. Initially, a webdriver is utilized to extract the links of the products from a specified number of pages, determined by the `max_pages` variable. The second part involves inputting a link, from which Heading, Model Number, SKU Number, Rating, Reviews, and Price of each product are extracted. Both functions are separately defined. To achieve an autonomous process of webscraping, the function that extracts links should be connected to the function that extracts data from the links, and the final output is stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e4394",
   "metadata": {},
   "source": [
    "### Product Link Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526ed1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "base_url = 'https://www.bestbuy.com/site/laptop-computers/all-laptops/pcmcat138500050001.c?id=pcmcat138500050001&intl=nosplash'\n",
    "driver.get(base_url)\n",
    "max_pages = 25\n",
    "collected_links = set()\n",
    "next_page_selector = \"a.sku-list-page-next[aria-disabled='false']\"\n",
    "current_page = 1\n",
    "while current_page <= max_pages:\n",
    "    try:\n",
    "        next_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, next_page_selector))\n",
    "        )\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        product_links = soup.find_all('a', class_='image-link')\n",
    "        for link in product_links:\n",
    "            href = link.get('href')\n",
    "            full_link = f'https://www.bestbuy.com{href}'\n",
    "            collected_links.add(full_link)\n",
    "        next_button.click()\n",
    "        time.sleep(5)\n",
    "        current_page += 1\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        break\n",
    "for link in collected_links:\n",
    "    print(link)\n",
    "num_links_collected = len(collected_links)\n",
    "print(f\"Number of links collected: {num_links_collected}\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278792b1",
   "metadata": {},
   "source": [
    "### Product Information Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = # add any URL extracted from the function above to test \n",
    "headers = {\n",
    "    'User-Agent': (\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "        '(KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    )\n",
    "}\n",
    "with requests.Session() as session:\n",
    "    try:\n",
    "        response = session.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        heading_element = soup.find('h1', class_='heading-5 v-fw-regular')\n",
    "        model_element = soup.find('div', class_='model product-data')\n",
    "        sku_element = soup.find('div', class_='sku product-data')\n",
    "        rating_reviews_element = soup.find('p', class_='visually-hidden')\n",
    "        price_element = soup.find('div', class_='priceView-hero-price priceView-customer-price')\n",
    "        if heading_element and heading_element.text:\n",
    "            heading_text = heading_element.text.strip()\n",
    "            print(f\"Heading: {heading_text}\")\n",
    "        if model_element and model_element.text:\n",
    "            model_text = model_element.text.strip()\n",
    "            print(f\"Model Number: {model_text}\")\n",
    "        if sku_element and sku_element.text:\n",
    "            sku_text = sku_element.text.strip()\n",
    "            print(f\"SKU Number: {sku_text}\")\n",
    "        if rating_reviews_element and rating_reviews_element.text:\n",
    "            rating_reviews_text = rating_reviews_element.text.strip()\n",
    "            match = re.search(r'([\\d.]+) out of 5 stars with (\\d+) reviews', rating_reviews_text)\n",
    "            if match:\n",
    "                rating, reviews = match.groups()\n",
    "                print(f\"Rating: {rating}\")\n",
    "                print(f\"Reviews: {reviews}\")\n",
    "            else:\n",
    "                print(\"Unable to extract rating and reviews.\")\n",
    "        if price_element and price_element.text:\n",
    "            price_match = re.search(r'\\$([\\d,.]+)', price_element.text.strip())\n",
    "            if price_match:\n",
    "                price_text = price_match.group(1).replace(',', '')\n",
    "                print(f\"Price: ${price_text}\")\n",
    "            else:\n",
    "                print(\"Unable to extract price.\")\n",
    "    except requests.exceptions.HTTPError as err:\n",
    "        print(f\"Failed to retrieve the page. HTTP error: {err}\")\n",
    "    except requests.exceptions.RequestException as err:\n",
    "        print(f\"Failed to retrieve the page. Error: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a234d20",
   "metadata": {},
   "source": [
    "By inputting the output of the first function into the second function and storing the result of the second function in a pandas dataframe, autonomous webscraping is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f89aeb",
   "metadata": {},
   "source": [
    "**------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
